import{a as C,b as I,c as S,d as T,e as w}from"./chunk-R6BOKDWR.js";import{a as E,c as M,h as _}from"./chunk-XUUUIT3D.js";import{B as x,y as v}from"./chunk-WMVF4ZO7.js";import{Ja as p,Ka as c,Lb as i,Ma as n,Mb as o,Ob as y,Pb as h,Za as f,pb as b,qb as u,rb as g,sb as s,tb as t,ub as e,vb as d}from"./chunk-NCBRYYCQ.js";function H(m,l){if(m&1&&(t(0,"a",9),i(1),e()),m&2){let r=l.$implicit;s("href",h(r.url),c),n(),o(r.name)}}function B(m,l){if(m&1&&(t(0,"a",9),i(1),e()),m&2){let r=l.$implicit;s("href",h(r.url),c),n(),o(r.name)}}var L=class m{experiment={name:"Unilateral Motor Imagery",description:"This experiment focuses on the <b>mental rehearsal of movement</b> involving a <b>single limb</b>\u2014typically the left or right hand\u2014without any actual muscular activity. It is designed to evaluate a participant\u2019s ability to generate <b>consistent</b> and <b>classifiable brain signals</b> through motor imagery alone. <b>Unilateral motor imagery</b> is widely used in <b>brain-computer interface (BCI) research</b> due to its simplicity, <b>clear cortical activation patterns</b>, and effectiveness in training <b>decoding algorithms</b>.",metadata:{type:"Unilateral",paradigm:"Endogenous",stimulus_type:"Visual",recording_modality:"EEG",control_mode:"Offline",difficulty_level:"Moderate",class_range:{min:2,max:4},intended_applications:[{name:"Rehabilitation",url:"/applications/rehabilitation"},{name:"BCI Training",url:"/applications/bci-training"},{name:"Prosthetics",url:"/applications/prosthetics"}],common_datasets:[{name:"BCI Competition IV 2a",url:"/datasets/bci-competition-iv-2a"},{name:"PhysioNet EEG Motor Imagery",url:"/datasets/physionet-eeg-mi"}]},objective:"The main objective of this experiment is to assess the capability of individuals to voluntarily modulate their <b>sensorimotor rhythms (SMRs)</b> through the imagined movement of a <b>single limb</b>, typically the <b>left or right hand</b>. By focusing on <b>unilateral motor imagery</b>, the protocol aims to isolate specific <b>cortical activation patterns</b> that can be detected using <b>non-invasive brain recording techniques</b> such as <b>EEG</b>. These patterns are then used to evaluate the feasibility of <b>decoding motor intentions</b> for <b>brain-computer interface (BCI)</b> applications.",hypothesis:"We hypothesize that <b>unilateral motor imagery</b>\u2014specifically the imagined movement of a <b>single hand</b>\u2014produces <b>distinguishable</b> and <b>repeatable changes</b> in the <b>mu and beta frequency bands</b> over <b>sensorimotor cortex</b>. These changes can be reliably detected through <b>EEG</b> and used to <b>classify mental states</b> corresponding to <b>left or right hand motor imagery</b>, supporting the development of <b>accurate and robust BCI control strategies</b>.",procedure:"Participants are seated comfortably in a <b>controlled environment</b> and fitted with <b>EEG equipment</b>. Each trial begins with a <b>fixation cross</b> displayed at the center of the screen, followed by a <b>visual cue</b> (such as an arrow) indicating which hand the participant should imagine moving. The participant is instructed to <b>imagine performing the movement</b> (e.g., grasping or rotating the hand) <b>without any actual muscle contraction</b>. Each imagery period typically lasts between <b>4 to 6 seconds</b> and is followed by a <b>rest interval</b>. The protocol is repeated across <b>multiple trials</b> to ensure sufficient data for analysis. The entire session lasts approximately <b>20 to 40 minutes</b> depending on the number of classes and blocks.",requirements:"This experiment requires access to a <b>modern web browser</b> capable of rendering visual stimuli with <b>precise timing</b> (e.g., Chrome or Firefox). For experimental use with EEG systems, the platform must support <b>synchronized stimulus presentation</b> and integration with <b>external acquisition hardware</b> via <b>serial, WebUSB</b>, or <b>network-based protocols</b>. Participants should be instructed on <b>motor imagery techniques</b> and perform the task in a <b>quiet, distraction-free environment</b>. Optional features include <b>online feedback interfaces</b> and <b>real-time monitoring tools</b> for data quality assessment.",characteristics:"This experiment is classified as <b>endogenous</b> because it relies entirely on the participant\u2019s <b>internal cognitive processes</b>\u2014specifically <b>voluntary mental imagery</b>\u2014to generate the desired neural activity. It is considered <b>unilateral</b> because only one limb (either the left or the right hand) is imagined per trial, allowing for <b>clearer separation</b> of cortical activation and <b>simplified classification</b> of motor intentions within brain-computer interface systems."};escapeHtml(l){let r=document.createElement("p");return r.innerText=l,r.innerHTML}static \u0275fac=function(r){return new(r||m)};static \u0275cmp=f({type:m,selectors:[["app-experiment"]],inputs:{experiment:"experiment"},decls:78,vars:15,consts:[[1,"page-container"],[1,"no-margin-bottom"],[3,"innerHTML"],["appearance","outlined"],[1,"grid-responsive-3"],["matListItemTitle",""],["matListItemLine",""],[1,"grid-responsive-2"],[1,"button-wrapper"],["matButton","","target","_blank",3,"href"]],template:function(r,a){r&1&&(t(0,"div",0)(1,"h1",1),i(2),e(),d(3,"p",2),t(4,"mat-card",3)(5,"mat-card-content")(6,"mat-list")(7,"div",4)(8,"mat-list-item")(9,"span",5),i(10,"Task Laterality:"),e(),t(11,"span",6),i(12),e()(),t(13,"mat-list-item")(14,"span",5),i(15,"Paradigm:"),e(),t(16,"span",6),i(17),e()(),t(18,"mat-list-item")(19,"span",5),i(20,"Stimulus Type:"),e(),t(21,"span",6),i(22),e()(),t(23,"mat-list-item")(24,"span",5),i(25,"Recording Modality:"),e(),t(26,"span",6),i(27),e()(),t(28,"mat-list-item")(29,"span",5),i(30,"Estimated Duration:"),e(),t(31,"span",6),i(32),e()(),t(33,"mat-list-item")(34,"span",5),i(35,"Difficulty Level:"),e(),t(36,"span",6),i(37),e()(),t(38,"mat-list-item")(39,"span",5),i(40,"Classes:"),e(),t(41,"span",6),i(42),e()()()(),t(43,"mat-list")(44,"div",7)(45,"mat-list-item",8)(46,"span",5),i(47,"Intended Applications:"),e(),t(48,"span",6),u(49,H,2,3,"a",9,b),e()(),t(51,"mat-list-item",8)(52,"span",5),i(53,"Common Datasets:"),e(),t(54,"span",6),u(55,B,2,3,"a",9,b),e()()()()()(),t(57,"div",7)(58,"div")(59,"h1",1),i(60,"Objective"),e(),d(61,"p",2),e(),t(62,"div")(63,"h1",1),i(64,"Hypothesis"),e(),d(65,"p",2),e(),t(66,"div")(67,"h1",1),i(68,"Procedure"),e(),d(69,"p",2),e(),t(70,"div")(71,"h1",1),i(72,"Requirements"),e(),d(73,"p",2),e()(),t(74,"div")(75,"h1",1),i(76,"Cognitive Task Characteristics"),e(),d(77,"p",2),e()()),r&2&&(n(2),o(a.experiment.name),n(),s("innerHTML",a.experiment.description,p),n(9),o(a.experiment.metadata.type),n(5),o(a.experiment.metadata.paradigm),n(5),o(a.experiment.metadata.stimulus_type),n(5),o(a.experiment.metadata.recording_modality),n(5),o(a.experiment.metadata.control_mode),n(5),o(a.experiment.metadata.difficulty_level),n(5),y("",a.experiment.metadata.class_range.min," up to ",a.experiment.metadata.class_range.max),n(7),g(a.experiment.metadata.intended_applications),n(6),g(a.experiment.metadata.common_datasets),n(6),s("innerHTML",a.experiment.objective,p),n(4),s("innerHTML",a.experiment.hypothesis,p),n(4),s("innerHTML",a.experiment.procedure,p),n(4),s("innerHTML",a.experiment.requirements,p),n(4),s("innerHTML",a.experiment.characteristics,p))},dependencies:[w,S,T,I,C,x,v,_,E,M],styles:["span[matListItemLine][_ngcontent-%COMP%]{color:var(--mat-sys-primary)}mat-list-item.button-wrapper[_ngcontent-%COMP%]{height:100%}.button-wrapper[_ngcontent-%COMP%]   a[matButton][_ngcontent-%COMP%]{margin-top:4px}.grid-container[_ngcontent-%COMP%]{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:16px}.two-columns[_ngcontent-%COMP%]{display:grid;grid-template-columns:repeat(2,minmax(250px,1fr));gap:32px}@media (max-width: 768px){.two-columns[_ngcontent-%COMP%]{grid-template-columns:1fr}}"]})};export{L as a};
